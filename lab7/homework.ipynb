{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0 ",
   "id": "4682659a26249768"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-01T23:41:09.760982Z",
     "start_time": "2025-12-01T23:41:08.536012Z"
    }
   },
   "source": [
    "import os.path\n",
    "from timeit import timeit\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:41:10.726832Z",
     "start_time": "2025-12-01T23:41:09.764796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ],
   "id": "ca60a334709330a6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:41:10.732090Z",
     "start_time": "2025-12-01T23:41:10.730260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "MiÅ‚oÅ›Ä‡ szczÄ™Å›liwa. Czy to jest normalne,\n",
    "czy to powaÅ¼ne, czy to poÅ¼yteczne -\n",
    "co Å›wiat ma z dwojga ludzi,\n",
    "ktÃ³rzy nie widzÄ… Å›wiata?\n",
    "\n",
    "WywyÅ¼szeni ku sobie bez Å¼adnej zasÅ‚ugi,\n",
    "pierwsi lepsi z miliona, ale przekonani,\n",
    "Å¼e tak staÄ‡ siÄ™ musiaÅ‚o - w nagrodÄ™ za co?\n",
    "za nic;\n",
    "Å›wiatÅ‚o pada znikÄ…d -\n",
    "dlaczego wÅ‚aÅ›nie na tych, a nie na innych?\n",
    "Czy to obraÅ¼a sprawiedliwoÅ›Ä‡? Tak.\n",
    "Czy to narusza troskliwie piÄ™trzone zasady,\n",
    "strÄ…cÄ… ze szczytu moraÅ‚? Narusza i strÄ…ca.\n",
    "\n",
    "SpÃ³jrzcie na tych szczÄ™Å›liwych:\n",
    "gdyby siÄ™ chociaÅ¼ maskowali trochÄ™,\n",
    "udawali zgnÄ™bienie krzepiÄ…c tym przyjaciÃ³Å‚!\n",
    "SÅ‚uchajcie, jak siÄ™ Å›miejÄ… - obraÅºliwie.\n",
    "Jakim jÄ™zykiem mÃ³wiÄ… - zrozumiaÅ‚ym na pozÃ³r.\n",
    "A te ich ceremonie, ceregiele,\n",
    "wymyÅ›lne obowiÄ…zki wzglÄ™dem siebie -\n",
    "wyglÄ…da to na zmowÄ™ za plecami ludzkoÅ›ci!\n",
    "\n",
    "Trudno nawet przewidzieÄ‡, do czego by doszÅ‚o,\n",
    "gdyby ich przykÅ‚ad daÅ‚ siÄ™ naÅ›ladowaÄ‡.\n",
    "Na co liczyÄ‡ by mogÅ‚y religie, poezje,\n",
    "o czym by pamiÄ™tano, czego zaniechano.\n",
    "kto by chciaÅ‚ zostaÄ‡ w krÄ™gu.\n",
    "\n",
    "MiÅ‚oÅ›Ä‡ szczÄ™Å›liwa. Czy to jest konieczne?\n",
    "Takt i rozsÄ…dek kaÅ¼Ä… milczeÄ‡ o niej\n",
    "jako skandalu z wysokich sfer Å»ycia.\n",
    "Wspaniale dziatki rodzÄ… siÄ™ bez jej pomocy.\n",
    "Przenigdy nie zdolaÅ‚aby zaludniÄ‡ ziemi,\n",
    "zdarza siÄ™ przecieÅ¼ rzadko.\n",
    "\n",
    "Niech ludzie nie znajÄ…cy miÅ‚oÅ›ci szczÄ™Å›liwej\n",
    "twierdzÄ…, Å¼e nigdzie nie ma miÅ‚oÅ›ci szczÄ™Å›liwej.\n",
    "\n",
    "Z tÄ… wiarÄ… lÅ¼ej im bÄ™dzie i Å¼yÄ‡, i umieraÄ‡.\"\"\"\n",
    "\n",
    "#  ÅºrÃ³dÅ‚o: https://poezja.org/wz/Wislawa_Szymborska/19/Milosc_szczesliwa"
   ],
   "id": "37804f75ff97778a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:41:10.777602Z",
     "start_time": "2025-12-01T23:41:10.774571Z"
    }
   },
   "cell_type": "code",
   "source": "encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")",
   "id": "d29a223cd9d6081e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:41:10.819812Z",
     "start_time": "2025-12-01T23:41:10.818439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import timeit\n",
    "import torch\n"
   ],
   "id": "615a9f1705245ba7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1",
   "id": "36a8ec8a3cb87a56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:41:10.865445Z",
     "start_time": "2025-12-01T23:41:10.863466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def no_optimizations():\n",
    "    model.train()\n",
    "    _ = model(**encoded_input)\n",
    "\n",
    "\n",
    "def only_eval():\n",
    "    model.eval()\n",
    "    _ = model(**encoded_input)\n",
    "\n",
    "\n",
    "def eval_no_grad():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(**encoded_input)\n",
    "\n",
    "def eval_inference():\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        _ = model(**encoded_input)"
   ],
   "id": "54ff23d627f85ad8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:02.313349Z",
     "start_time": "2025-12-01T23:41:10.907707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_runs = 100\n",
    "\n",
    "no_opti_avg_time = timeit.timeit(no_optimizations, number=n_runs) / n_runs\n",
    "only_eval_avg_time = timeit.timeit(only_eval, number=n_runs) / n_runs\n",
    "eval_no_grad_avg_time = timeit.timeit(eval_no_grad, number=n_runs) / n_runs\n",
    "eval_inference_avg_time = timeit.timeit(eval_inference, number=n_runs) / n_runs"
   ],
   "id": "a1e7bafc9743022c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:02.320109Z",
     "start_time": "2025-12-01T23:42:02.318400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Benchmark results:\")\n",
    "print(f\"{no_opti_avg_time=:.6f}\")\n",
    "print(f\"{only_eval_avg_time=:.6f}\")\n",
    "print(f\"{eval_no_grad_avg_time=:.6f}\")\n",
    "print(f\"{eval_inference_avg_time=:.6f}\")"
   ],
   "id": "642309fa9a693bf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark results:\n",
      "no_opti_avg_time=0.144187\n",
      "only_eval_avg_time=0.126625\n",
      "eval_no_grad_avg_time=0.125176\n",
      "eval_inference_avg_time=0.118044\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:02.686832Z",
     "start_time": "2025-12-01T23:42:02.362590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Speedup comparing to no optimization method\")\n",
    "print(f\"{no_opti_avg_time / only_eval_avg_time:.2f}\")\n",
    "print(f\"{no_opti_avg_time / eval_no_grad_avg_time:.2f}\")\n",
    "print(f\"{no_opti_avg_time / eval_inference_avg_time:.2f}\")"
   ],
   "id": "5106d4c3cecc5568",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup comparing to no optimization method\n",
      "1.14\n",
      "1.15\n",
      "1.22\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2",
   "id": "3bc1059d29485db1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:02.695422Z",
     "start_time": "2025-12-01T23:42:02.693798Z"
    }
   },
   "cell_type": "code",
   "source": "from time import time",
   "id": "f3c81f6bb157d32c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:09.410844Z",
     "start_time": "2025-12-01T23:42:02.738903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "compiled_model = torch.compile(model)\n",
    "_ = compiled_model(**encoded_input)\n",
    "\n",
    "end_time = time() - start_time\n",
    "\n",
    "print(f\"Total time of compilation and warmup inference: {end_time:.6f}\")"
   ],
   "id": "732182ae84b50dda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of compilation and warmup inference: 6.669413\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:09.455461Z",
     "start_time": "2025-12-01T23:42:09.453831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compiled_eval_inference():\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_model(**encoded_input)"
   ],
   "id": "ffc82248d9185902",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:22.541683Z",
     "start_time": "2025-12-01T23:42:09.497056Z"
    }
   },
   "cell_type": "code",
   "source": "compiled_eval_inference_avg_time = timeit.timeit(compiled_eval_inference, number=n_runs) / n_runs",
   "id": "7735378bd86b3ad4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:22.547511Z",
     "start_time": "2025-12-01T23:42:22.545845Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"{compiled_eval_inference_avg_time=:.6f}\")",
   "id": "a4861d2f67b59914",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiled_eval_inference_avg_time=0.130274\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:22.592185Z",
     "start_time": "2025-12-01T23:42:22.590419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Speedup comparing to no optimization method\")\n",
    "print(f\"{no_opti_avg_time / compiled_eval_inference_avg_time:.2f}\")"
   ],
   "id": "33f26248803af74b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup comparing to no optimization method\n",
      "1.11\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Yes. This is the best so far! ðŸ’…ðŸ’… (this is Kuba, no LLM here)",
   "id": "fb83241a8b90b4bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3",
   "id": "6135607a49ace60b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:22.637373Z",
     "start_time": "2025-12-01T23:42:22.634769Z"
    }
   },
   "cell_type": "code",
   "source": "model = model.to(\"cpu\")",
   "id": "d3713608a2d4207",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:23.033280Z",
     "start_time": "2025-12-01T23:42:22.678123Z"
    }
   },
   "cell_type": "code",
   "source": "model_quantized = torch.ao.quantization.quantize_dynamic(model, dtype=torch.qint8)",
   "id": "3ea98135f8bcbf47",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:23.141Z",
     "start_time": "2025-12-01T23:42:23.037598Z"
    }
   },
   "cell_type": "code",
   "source": "print(model_quantized)",
   "id": "d67e300ff0b27dc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNetModel(\n",
      "  (embeddings): MPNetEmbeddings(\n",
      "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): MPNetEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x MPNetLayer(\n",
      "        (attention): MPNetAttention(\n",
      "          (attn): MPNetSelfAttention(\n",
      "            (q): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (k): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (o): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (intermediate): MPNetIntermediate(\n",
      "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): MPNetOutput(\n",
      "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (relative_attention_bias): Embedding(32, 12)\n",
      "  )\n",
      "  (pooler): MPNetPooler(\n",
      "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:23.525118Z",
     "start_time": "2025-12-01T23:42:23.148044Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), \"model.pth\")",
   "id": "6acbb1545c43be78",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:23.815119Z",
     "start_time": "2025-12-01T23:42:23.528274Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model_quantized.state_dict(), \"model_quantized.pth\")",
   "id": "2484da844d0c504f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:23.821118Z",
     "start_time": "2025-12-01T23:42:23.819267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Size of normal model: {os.path.getsize('model.pth') / 1024 / 1024:.5} MB\")\n",
    "print(f\"Size of model quantized: {os.path.getsize('model_quantized.pth') / 1024 / 1024:.5} MB\")"
   ],
   "id": "5aecfc3ad69a57f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of normal model: 417.72 MB\n",
      "Size of model quantized: 173.1 MB\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:23.866204Z",
     "start_time": "2025-12-01T23:42:23.864372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def quantized_eval_inference():\n",
    "    model_quantized.eval()\n",
    "    with torch.inference_mode():\n",
    "        _ = model_quantized(**encoded_input)"
   ],
   "id": "8c5b573006cfecc8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:29.548080Z",
     "start_time": "2025-12-01T23:42:23.908121Z"
    }
   },
   "cell_type": "code",
   "source": "quantized_eval_inference_avg_time = timeit.timeit(quantized_eval_inference, number=n_runs) / n_runs",
   "id": "daebb82107e291a3",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:29.554221Z",
     "start_time": "2025-12-01T23:42:29.552527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Benchmark results:\")\n",
    "print(f\"{no_opti_avg_time=:.6f}\")\n",
    "print(f\"{quantized_eval_inference_avg_time=:.6f}\")"
   ],
   "id": "953b6b8ee25b51be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark results:\n",
      "no_opti_avg_time=0.144187\n",
      "quantized_eval_inference_avg_time=0.056384\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:29.599729Z",
     "start_time": "2025-12-01T23:42:29.597897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Speedup comparing to no optimization method\")\n",
    "print(f\"{no_opti_avg_time / quantized_eval_inference_avg_time:.2f}\")"
   ],
   "id": "3c60de1d2c76f39e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup comparing to no optimization method\n",
      "2.56\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Quantization helps us speed up model 2 times and help us save 2.5 times disc space (250MB), but I dont have any metrics to compare quality so I cant tell",
   "id": "5f88b315312457b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4",
   "id": "f62c0536110adb43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:30.111227Z",
     "start_time": "2025-12-01T23:42:29.642738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ],
   "id": "50af3c22182f3c3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:30.185028Z",
     "start_time": "2025-12-01T23:42:30.117969Z"
    }
   },
   "cell_type": "code",
   "source": "model_gpu = model.to(device)",
   "id": "bef922cb9ef9fc69",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:30.267123Z",
     "start_time": "2025-12-01T23:42:30.188102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_input_gpu = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "with torch.inference_mode():\n",
    "    outputs = model_gpu(**encoded_input_gpu)"
   ],
   "id": "9a5b0389d99d14cb",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:42:30.274288Z",
     "start_time": "2025-12-01T23:42:30.271582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "compiled_model_gpu = torch.compile(model_gpu)\n",
    "compiled_max_autotune_model_gpu = torch.compile(model_gpu, mode=\"max-autotune\")\n",
    "compiled_max_autotune_no_cudagraphs_model_gpu = torch.compile(model_gpu, mode=\"max-autotune-no-cudagraphs\")"
   ],
   "id": "881f6616075b998c",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:48:44.042755Z",
     "start_time": "2025-12-01T23:48:44.038896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compiled_model_gpu_eval_inference():\n",
    "    compiled_model_gpu.eval()\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_model_gpu(**encoded_input_gpu)\n",
    "\n",
    "\n",
    "def compiled_max_autotune_model_gpu_eval_inference():\n",
    "    compiled_max_autotune_model_gpu.eval()\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_max_autotune_model_gpu(**encoded_input_gpu)\n",
    "\n",
    "\n",
    "def compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference():\n",
    "    compiled_max_autotune_no_cudagraphs_model_gpu.eval()\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_max_autotune_no_cudagraphs_model_gpu(**encoded_input_gpu)\n",
    "\n",
    "\n",
    "def compiled_model_gpu_eval_inference_and_input_to_gpu():\n",
    "    compiled_model_gpu.eval()\n",
    "    encoded_input_gpu = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_model_gpu(**encoded_input_gpu)\n",
    "\n",
    "\n",
    "def compiled_max_autotune_model_gpu_eval_inference_and_input_to_gpu():\n",
    "    compiled_max_autotune_model_gpu.eval()\n",
    "    encoded_input_gpu = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_max_autotune_model_gpu(**encoded_input_gpu)\n",
    "\n",
    "\n",
    "def compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_and_input_to_gpu():\n",
    "    compiled_max_autotune_no_cudagraphs_model_gpu.eval()\n",
    "    encoded_input_gpu = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_max_autotune_no_cudagraphs_model_gpu(**encoded_input_gpu)"
   ],
   "id": "8d3e780c018f1ca6",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:51:38.814054Z",
     "start_time": "2025-12-01T23:51:38.812333Z"
    }
   },
   "cell_type": "code",
   "source": "n_runs = 1000",
   "id": "23a8e8fa0d19bd83",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:53:16.298417Z",
     "start_time": "2025-12-01T23:51:39.923448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "compiled_model_gpu_eval_inference_avg_time = timeit.timeit(compiled_model_gpu_eval_inference, number=n_runs) / n_runs\n",
    "compiled_max_autotune_model_gpu_eval_inference_avg_time = timeit.timeit(compiled_max_autotune_model_gpu_eval_inference, number=n_runs) / n_runs\n",
    "compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_avg_time = timeit.timeit(compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference, number=n_runs) / n_runs\n",
    "\n",
    "# calculating input to gpu too\n",
    "compiled_model_gpu_eval_inference_input_to_gpu_avg_time = timeit.timeit(compiled_model_gpu_eval_inference_and_input_to_gpu, number=n_runs) / n_runs\n",
    "compiled_max_autotune_model_gpu_eval_inference_input_to_gpu_avg_time = timeit.timeit(compiled_max_autotune_model_gpu_eval_inference_and_input_to_gpu, number=n_runs) / n_runs\n",
    "compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_input_to_gpu_avg_time = timeit.timeit(compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_and_input_to_gpu, number=n_runs) / n_runs"
   ],
   "id": "1d8bed89237d6445",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:53:16.313623Z",
     "start_time": "2025-12-01T23:53:16.311533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Benchmark results:\")\n",
    "print(f\"{compiled_model_gpu_eval_inference_avg_time=:.6f}\")\n",
    "print(f\"{compiled_max_autotune_model_gpu_eval_inference_avg_time=:.6f}\")\n",
    "print(f\"{compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_avg_time=:.6f}\")\n",
    "\n",
    "print(\"\\nWith inputs to gpu time included:\")\n",
    "print(f\"{compiled_model_gpu_eval_inference_input_to_gpu_avg_time=:.6f}\")\n",
    "print(f\"{compiled_max_autotune_model_gpu_eval_inference_input_to_gpu_avg_time=:.6f}\")\n",
    "print(f\"{compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_input_to_gpu_avg_time=:.6f}\")"
   ],
   "id": "26c1dd2c117c9895",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark results:\n",
      "compiled_model_gpu_eval_inference_avg_time=0.015556\n",
      "compiled_max_autotune_model_gpu_eval_inference_avg_time=0.015493\n",
      "compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_avg_time=0.015649\n",
      "\n",
      "With inputs to gpu time included:\n",
      "compiled_model_gpu_eval_inference_input_to_gpu_avg_time=0.016106\n",
      "compiled_max_autotune_model_gpu_eval_inference_input_to_gpu_avg_time=0.017008\n",
      "compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_input_to_gpu_avg_time=0.016561\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:53:35.201399Z",
     "start_time": "2025-12-01T23:53:35.198544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Speedup comparing to no optimization method\")\n",
    "print(f\"{no_opti_avg_time / compiled_model_gpu_eval_inference_avg_time:.2f}\")\n",
    "print(f\"{no_opti_avg_time / compiled_max_autotune_model_gpu_eval_inference_avg_time:.2f}\")\n",
    "print(f\"{no_opti_avg_time / compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_avg_time:.2f}\")\n",
    "\n",
    "print(\"\\nWith inputs to gpu time included:\")\n",
    "print(f\"{no_opti_avg_time / compiled_model_gpu_eval_inference_input_to_gpu_avg_time:.2f}\")\n",
    "print(f\"{no_opti_avg_time / compiled_max_autotune_model_gpu_eval_inference_input_to_gpu_avg_time:.2f}\")\n",
    "print(f\"{no_opti_avg_time / compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_input_to_gpu_avg_time:.2f}\")"
   ],
   "id": "a9e5134c78f091e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup comparing to no optimization method\n",
      "9.27\n",
      "9.31\n",
      "9.21\n",
      "\n",
      "With inputs to gpu time included:\n",
      "8.95\n",
      "8.48\n",
      "8.71\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:03:15.109914Z",
     "start_time": "2025-12-02T00:03:15.107985Z"
    }
   },
   "cell_type": "code",
   "source": "device",
   "id": "6d48a6ba8bada87d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:57:07.572851Z",
     "start_time": "2025-12-01T23:57:07.564009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text2 = \"Morgulium\"\n",
    "encoded_input2 = tokenizer(text2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "encoded_input2_gpu = {k: v.to(device) for k, v in encoded_input2.items()}"
   ],
   "id": "4719cac3ee8a1b6f",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:00:17.461873Z",
     "start_time": "2025-12-02T00:00:17.458673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compiled_model_gpu_eval_inference():\n",
    "    compiled_model_gpu.eval()\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_model_gpu(**encoded_input2_gpu)\n",
    "\n",
    "\n",
    "def compiled_max_autotune_model_gpu_eval_inference():\n",
    "    compiled_max_autotune_model_gpu.eval()\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_max_autotune_model_gpu(**encoded_input2_gpu)\n",
    "\n",
    "\n",
    "def compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference():\n",
    "    compiled_max_autotune_no_cudagraphs_model_gpu.eval()\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_max_autotune_no_cudagraphs_model_gpu(**encoded_input2_gpu)\n",
    "\n",
    "\n",
    "def compiled_model_gpu_eval_inference_and_input_to_gpu():\n",
    "    compiled_model_gpu.eval()\n",
    "    encoded_input_gpu2 = {k: v.to(device) for k, v in encoded_input2.items()}\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_model_gpu(**encoded_input_gpu2)\n",
    "\n",
    "\n",
    "def compiled_max_autotune_model_gpu_eval_inference_and_input_to_gpu():\n",
    "    compiled_max_autotune_model_gpu.eval()\n",
    "    encoded_input_gpu2 = {k: v.to(device) for k, v in encoded_input2.items()}\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_max_autotune_model_gpu(**encoded_input_gpu2)\n",
    "\n",
    "\n",
    "def compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_and_input_to_gpu():\n",
    "    compiled_max_autotune_no_cudagraphs_model_gpu.eval()\n",
    "    encoded_input_gpu2 = {k: v.to(device) for k, v in encoded_input2.items()}\n",
    "    with torch.inference_mode():\n",
    "        _ = compiled_max_autotune_no_cudagraphs_model_gpu(**encoded_input_gpu2)"
   ],
   "id": "c293e415c7ee204c",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:00:29.333684Z",
     "start_time": "2025-12-02T00:00:17.706634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "compiled_model_gpu_eval_inference_avg_time = timeit.timeit(compiled_model_gpu_eval_inference, number=n_runs) / n_runs\n",
    "compiled_max_autotune_model_gpu_eval_inference_avg_time = timeit.timeit(compiled_max_autotune_model_gpu_eval_inference, number=n_runs) / n_runs\n",
    "compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_avg_time = timeit.timeit(compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference, number=n_runs) / n_runs\n",
    "\n",
    "# calculating input to gpu too\n",
    "compiled_model_gpu_eval_inference_input_to_gpu_avg_time = timeit.timeit(compiled_model_gpu_eval_inference_and_input_to_gpu, number=n_runs) / n_runs\n",
    "compiled_max_autotune_model_gpu_eval_inference_input_to_gpu_avg_time = timeit.timeit(compiled_max_autotune_model_gpu_eval_inference_and_input_to_gpu, number=n_runs) / n_runs\n",
    "compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_input_to_gpu_avg_time = timeit.timeit(compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_and_input_to_gpu, number=n_runs) / n_runs"
   ],
   "id": "b16e8f65f580405b",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:55:50.072376Z",
     "start_time": "2025-12-01T23:55:50.070340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Benchmark results (small input):\")\n",
    "print(f\"{compiled_model_gpu_eval_inference_avg_time=:.6f}\")\n",
    "print(f\"{compiled_max_autotune_model_gpu_eval_inference_avg_time=:.6f}\")\n",
    "print(f\"{compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_avg_time=:.6f}\")\n",
    "\n",
    "print(\"\\nWith inputs to gpu time included:\")\n",
    "print(f\"{compiled_model_gpu_eval_inference_input_to_gpu_avg_time=:.6f}\")\n",
    "print(f\"{compiled_max_autotune_model_gpu_eval_inference_input_to_gpu_avg_time=:.6f}\")\n",
    "print(f\"{compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_input_to_gpu_avg_time=:.6f}\")"
   ],
   "id": "3d3420de9a480a78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark results (small input):\n",
      "compiled_model_gpu_eval_inference_avg_time=0.010158\n",
      "compiled_max_autotune_model_gpu_eval_inference_avg_time=0.010410\n",
      "compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_avg_time=0.009178\n",
      "\n",
      "With inputs to gpu time included:\n",
      "compiled_model_gpu_eval_inference_input_to_gpu_avg_time=0.002147\n",
      "compiled_max_autotune_model_gpu_eval_inference_input_to_gpu_avg_time=0.002019\n",
      "compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_input_to_gpu_avg_time=0.002232\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T23:55:54.908123Z",
     "start_time": "2025-12-01T23:55:54.905483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Speedup comparing to no optimization method (small input)\")\n",
    "print(f\"{no_opti_avg_time / compiled_model_gpu_eval_inference_avg_time:.2f}\")\n",
    "print(f\"{no_opti_avg_time / compiled_max_autotune_model_gpu_eval_inference_avg_time:.2f}\")\n",
    "print(f\"{no_opti_avg_time / compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_avg_time:.2f}\")\n",
    "\n",
    "print(\"\\nWith inputs to gpu time included:\")\n",
    "print(f\"{no_opti_avg_time / compiled_model_gpu_eval_inference_input_to_gpu_avg_time:.2f}\")\n",
    "print(f\"{no_opti_avg_time / compiled_max_autotune_model_gpu_eval_inference_input_to_gpu_avg_time:.2f}\")\n",
    "print(f\"{no_opti_avg_time / compiled_max_autotune_no_cudagraphs_model_gpu_eval_inference_input_to_gpu_avg_time:.2f}\")"
   ],
   "id": "76100c739bbfed9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup comparing to no optimization method (small input)\n",
      "14.19\n",
      "13.85\n",
      "15.71\n",
      "\n",
      "With inputs to gpu time included:\n",
      "67.15\n",
      "71.40\n",
      "64.59\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I dont understand. Why when we perform more operations (we count time when we convert input to gpu) we got so much faster times. I was looking at it and I do not understand. Also I did not notice any difference between mode. Time is sooo small so that different run gave me different results",
   "id": "19d2a4d90273f14b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5",
   "id": "e02071c37c101909"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:37:44.098686Z",
     "start_time": "2025-12-02T00:37:43.519817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# I will reset my model because I broke something XD\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model_fresh = AutoModel.from_pretrained(model_name).to(device)"
   ],
   "id": "4b09093335df31a3",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:37:44.521547Z",
     "start_time": "2025-12-02T00:37:44.519447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "capability = torch.cuda.get_device_capability()\n",
    "print(f\"CUDA device capability: {capability}\")\n",
    "\n",
    "# Tensor Cores are available on NVidia GPUs with CUDA >= 7 (e.g. Volta, Turing, Ampere, Hopper)\n",
    "if capability >= (7, 0):\n",
    "    print(\"Tensor Cores available: fast float16 supported.\")\n",
    "else:\n",
    "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")"
   ],
   "id": "729703443939a100",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device capability: (7, 5)\n",
      "Tensor Cores available: fast float16 supported.\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:37:45.075128Z",
     "start_time": "2025-12-02T00:37:45.069466Z"
    }
   },
   "cell_type": "code",
   "source": "model_half = model.half().to(device)",
   "id": "e72d30aa0bd4002c",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:37:45.431107Z",
     "start_time": "2025-12-02T00:37:45.429267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = encoded_input[\"input_ids\"]\n",
    "attention_mask = encoded_input[\"attention_mask\"]"
   ],
   "id": "7b206c87ca71692f",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:37:45.758770Z",
     "start_time": "2025-12-02T00:37:45.749979Z"
    }
   },
   "cell_type": "code",
   "source": "_ = model_half(**encoded_input_gpu)",
   "id": "1a1ad0a4bc4fc1ad",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:37:46.472721Z",
     "start_time": "2025-12-02T00:37:46.470671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(next(model_fresh.parameters()).dtype)\n",
    "print(next(model.parameters()).dtype)\n",
    "print(next(model_half.parameters()).dtype)"
   ],
   "id": "c74a0a51320b0c0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float16\n",
      "torch.float16\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### model_half = model.half().to(device) is making model half precision too!!!!!",
   "id": "1650c7fe40b376b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:38:48.642872Z",
     "start_time": "2025-12-02T00:38:48.640533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.amp import autocast\n",
    "\n",
    "def f32_eval_inference():\n",
    "    model_fresh.eval()\n",
    "    with torch.inference_mode():\n",
    "        _ = model_fresh(**encoded_input_gpu)\n",
    "\n",
    "\n",
    "def f16_eval_inference():\n",
    "    model_half.eval()\n",
    "    with torch.inference_mode():\n",
    "        _ = model_half(**encoded_input_gpu)\n",
    "\n",
    "\n",
    "def autocast_eval_inference():\n",
    "    model_fresh.eval()\n",
    "    with torch.inference_mode():\n",
    "        with autocast(device_type=device, dtype=torch.float16):\n",
    "            _ = model_fresh(**encoded_input_gpu)"
   ],
   "id": "c74a10453d84d115",
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:38:52.812370Z",
     "start_time": "2025-12-02T00:38:48.958543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_runs = 100\n",
    "\n",
    "f32_eval_inference_avg_time = timeit.timeit(f32_eval_inference, number=n_runs) / n_runs\n",
    "f16_eval_inference_avg_time = timeit.timeit(f16_eval_inference, number=n_runs) / n_runs\n",
    "autocast_eval_inference_avg_time = timeit.timeit(autocast_eval_inference, number=n_runs) / n_runs"
   ],
   "id": "2da938961cb75e6e",
   "outputs": [],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:38:55.358337Z",
     "start_time": "2025-12-02T00:38:55.356245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Benchmark results:\")\n",
    "print(f\"{f32_eval_inference_avg_time=:.6f}\")\n",
    "print(f\"{f16_eval_inference_avg_time=:.6f}\")\n",
    "print(f\"{autocast_eval_inference_avg_time=:.6f}\")"
   ],
   "id": "d436af4e51d00372",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark results:\n",
      "f32_eval_inference_avg_time=0.019158\n",
      "f16_eval_inference_avg_time=0.007637\n",
      "autocast_eval_inference_avg_time=0.011719\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:39:21.586384Z",
     "start_time": "2025-12-02T00:39:21.584089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Speedup comparing to no optimization method\")\n",
    "print(f\"{no_opti_avg_time / f32_eval_inference_avg_time:.2f}\")\n",
    "print(f\"{no_opti_avg_time / f16_eval_inference_avg_time:.2f}\")\n",
    "print(f\"{no_opti_avg_time / autocast_eval_inference_avg_time:.2f}\")"
   ],
   "id": "3086c758d8c0f3a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speedup comparing to no optimization method\n",
      "7.53\n",
      "18.88\n",
      "12.30\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I will check metrics to choose correct model for my case. Autocast seems cool",
   "id": "711ff47b62c6a0da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6",
   "id": "ced4a1463ddc9da8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "So I have chosen sleep (yet again)",
   "id": "5f6f11420a906b3e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
